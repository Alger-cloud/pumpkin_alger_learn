
## 吃瓜教程组队学习--Task01

### 1、Task01--已了解到的内容
#### 1.1 对机器学习的理解：
&emsp;&emsp;机器学习是基于对训练数据的学习，更多的是基于假设来‘总结经验’，形成一套新的模型，或者说是对传统算法的学习，人为给定一些假设（可以把这些假设看成经验），总结一套新的算法。
#### 1.2 基本概念：
&emsp;&emsp;(1) 有数据集(dataset)，样本(sample)，特征(feature)，维数(dimensionality)，训练样本(training data)，假设(hypothesis)等等。
&emsp;&emsp;(2) 回归(regression)和分类(classification)
&emsp;&emsp;(3) 监督学习(supervised learning，如回归)和无监督学习(unsupervised learning，如分类，聚类)
&emsp;&emsp;(4) 泛化能力(generalization)
#### 1.3 归纳偏好：
&emsp;&emsp;>>>机器学习算法在学习过程中对某种类型的假设的偏好，称为‘归纳偏好’，简单理解就是多次模型对同一个测试集（预测集）算出的结果要一样，要一直基于其中的某个‘偏好’来得出结果(不知道算不算理解到位，哈哈)。
&emsp;&emsp;其中设置偏好的原则可以是：‘奥卡姆剃刀’原则：同一个预测精度，选算法最简单那个。记住：但是‘奥卡姆剃刀’原则不是在所有地方适用(意思是在大多数情况适用？有点疑惑，这里)。
&emsp;&emsp;通过NFT定理(‘没有免费的误差’定理)可证，‘聪明算法’和‘瞎猜算法’的总误差一样？(我学会机器学习了-》瞎猜就行，哈哈)
&emsp;&emsp;等等，难道真的想半天弄出的一个‘聪明算法’和‘瞎猜算法’(后面统称cc，即猜猜)效果一样，不，哪些地方肯定不对，过程应该没有太大问题，所以回到最初的假设，
&emsp;&emsp;引用西瓜书内容--《我们需注意到， NFL 定理有一个重要前提:所有"问题"出现的机会相同、或所有问题同等重要.但实际情形并不是这样.很多时候，我们只关注自己正在试图解决的问题(例如某个具体应用任务)，希望为它找到一个解决方案，至于这个解决方案在别的问题、甚至在相似的问题上是否为好方案，我们并不关心.例如，为了快速从 地到达 地，如果我们正在考虑的 地是南京鼓·楼、 地是南京新街口，那么"骑自行车"是很好的解决方案;这个方案对地是南京鼓楼、 地是北京新街口的情形显然很糟糕，但我们对此并不关心.》
&emsp;&emsp;对上面的上面那句话是，任何问题本身就有‘特殊性’，那么该问题的‘最优解’也具有‘特殊性’，或者换句话说，具体问题具体分析，也就是说该本题偏向‘骑自行车’，所以，你如果随便选择一种交通工具的话，很难得到最优的解，但是你有经验（偏好）的话，你能更大概率选出最优解。




### 2、Task01--疑惑不懂的内容
&emsp;&emsp;(1) 假设空间，A,B,C三个样本分别有3,2,2种可能，那么假设空间的规模：4*3*3+1=37个-？

### 目前只看到这里了，下次接着看
(其实是去看guozuvsshatele)