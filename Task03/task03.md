## 第四章 决策树
&emsp;&emsp;(1)决策树又可称"判定树"根据上下文，**"决策树"有时是指学习方法，有时是指学得的树。**
&emsp;&emsp;(2)决策树(decision tree) 是一类常见的机器学习方法.以二分类任务为例，一般希望从给定训练数据集学得一个模型用以对新示例进行分类。

### 4.1 基本流程

&emsp;&emsp;(1)决策过程中提出的每个判定问题都是对某个属性的"测试"。
&emsp;&emsp;(2)一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点;叶结点对应于决策结果；其他每个结点则对应于一个属性测试。
&emsp;&emsp;(3)根结点包含样本全集.从根结点到每个叶结点的路径对应了一个判定测试序列。
&emsp;&emsp;(4)策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。
&emsp;&emsp;**(5)其基本流程遵循简单且直观的"分而治之" (divide-and-conquer) 策略。**
&emsp;&emsp;**(6)决策树的生成是一个递归过程。**

### 4.2 划分选择
&emsp;&emsp;信息熵(information entropy) 是度量样本集合纯度最常用的一种指标.
&emsp;&emsp;我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的"纯度" (purity) 越来越高.
$$
Ent(D) = -\sum_{k=1}^{|y|}p_{k}log{2}{p_{k}}
$$
&emsp;&emsp;Ent(D) 的值越小，则 的纯度越高.

#### **4.2.1 信息增益Gain**
$$
Gain(D,a)=Ent(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)
$$
&emsp;&emsp;一般而言，信息增益越大，则意味着使周属性a来进行划分所获得的"纯度提升"越大.因此，我们可用信息增益来进行决策树的划分属性选择.
#### **4.2.2 增益率Gain**
&emsp;&emsp;实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种
偏好可能带来的不利影响，著名的 C4.5 决策树算法 [Quinlan 1993J 不直接使用信息增益，而是使用"增益率" (gain ratio) 来选择最优划分属性.采用与前面式子相同的符号表示，增益率定义为
$$
Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)} 
$$
其中IV(a)称为属性 的"固有值" (intrinsic value) 
$$
IV(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$

#### 4.2.3 基尼指数
&emsp;&emsp;CART 决策树使用"基尼指数" (Gini index) 来选择划分属性.采用与式(4.1) 相同的符号，数据集 的纯度可用基尼值来度量:
$$
Gini(D) = \sum_{k=1}^{|y|}\sum_{k'≠k}p_kp_{k'}
=1-\sum_{k=1}^{|y|}p_k^2
$$
&emsp;&emsp;直观来说， Gini(D) 反映了从数据集 中随机抽取两个样本，其类别标记不一致的概率.因此， Gini(D) 越小，则数据集 的纯度越高.
&emsp;&emsp;同理，属性 的基尼指数定义为：
$$
Gini\_index(D,a) = \sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)
$$


###### 参考：周志华《机器学习》

